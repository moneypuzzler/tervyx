"""
Enhanced TERVYX Pipeline with Author Metadata and Reproducibility Focus
====================================================================

Updated pipeline that:
1. Minimizes AI API usage (only for essential abstract analysis)
2. Automatically includes author metadata and DOI information
3. Ensures reproducibility and proper academic attribution
4. Implements loop reinforcement strategy
"""

import asyncio
import os
import json
from typing import Dict, List, Optional, Any
from datetime import datetime
import traceback

# Import system components
from .pubmed_integration import PubMedAPI, PubMedPaper
from .ai_abstract_analyzer import AnalysisAggregator
from .cost_optimized_analyzer import CostOptimizedAnalyzer
from .journal_quality_db import JournalQualityDatabase
from .real_meta_analysis import RealMetaAnalyzer, generate_real_tervyx_entry
from .author_metadata import get_standardized_metadata, AUTHOR_METADATA

class EnhancedTERVYXPipeline:
    """
    Enhanced TERVYX pipeline with focus on reproducibility and proper attribution
    """
    
    def __init__(self, 
                 email: str,
                 gemini_api_key: str,
                 ncbi_api_key: Optional[str] = None,
                 minimize_ai_usage: bool = True):
        """
        Initialize enhanced pipeline with reproducibility focus
        """
        
        self.email = email
        self.gemini_api_key = gemini_api_key
        self.ncbi_api_key = ncbi_api_key
        self.minimize_ai_usage = minimize_ai_usage
        
        # Initialize components
        self.pubmed_api = PubMedAPI(email, api_key=ncbi_api_key)
        
        # AI analyzer - only used for essential abstract analysis
        if minimize_ai_usage:
            print("üî¨ Initializing cost-optimized AI analyzer (minimal usage mode)")
            self.ai_analyzer = CostOptimizedAnalyzer(
                api_key=gemini_api_key,
                enable_cost_tracking=True
            )
        else:
            self.ai_analyzer = CostOptimizedAnalyzer(gemini_api_key)
        
        self.analysis_aggregator = AnalysisAggregator()
        self.journal_db = JournalQualityDatabase()
        self.meta_analyzer = RealMetaAnalyzer()
        
        # Enhanced configuration for reproducibility
        self.config = {
            'max_papers_search': 100,
            'max_papers_analyze': 30,
            'min_papers_meta_analysis': 3,
            'analysis_timeout_minutes': 30,
            'reproducibility_mode': True,
            'strict_quality_control': True,
            'ai_usage_logging': True
        }
        
        # Metadata for all entries
        self.author_metadata = get_standardized_metadata()
        
        print(f"üéØ Enhanced TERVYX Pipeline initialized")
        print(f"üìß Contact: {AUTHOR_METADATA.author_info['email']}")
        print(f"üåê Website: {AUTHOR_METADATA.author_info['website']}")
        print(f"üë§ Author: {AUTHOR_METADATA.author_info['name_english']} ({AUTHOR_METADATA.author_info['name_korean']})")
    
    async def generate_entry_with_attribution(self, substance: str, outcome_category: str) -> Dict[str, Any]:
        """
        Generate TERVYX entry with full author attribution and reproducibility metadata
        """
        start_time = datetime.now()
        
        try:
            print(f"\nüöÄ Enhanced TERVYX Analysis: {substance} + {outcome_category}")
            print(f"‚è∞ Started at: {start_time.isoformat()}")
            print(f"üë§ Generated by: {AUTHOR_METADATA.author_info['name_english']} ({AUTHOR_METADATA.author_info['name_korean']})")
            
            # Step 1: Search PubMed (no AI needed)
            print(f"\nüìö Step 1: Searching PubMed literature...")
            
            pmids = await self.pubmed_api.search_papers(
                substance=substance,
                outcome=outcome_category,
                max_results=self.config['max_papers_search']
            )
            
            if not pmids:
                return self._create_error_entry(
                    error='No papers found in PubMed search',
                    substance=substance,
                    outcome_category=outcome_category,
                    search_results=0
                )
            
            print(f"‚úÖ Found {len(pmids)} papers")
            
            # Step 2: Fetch metadata (no AI needed)
            print(f"\nüìÑ Step 2: Fetching detailed metadata...")
            
            papers = await self.pubmed_api.fetch_detailed_metadata(
                pmids[:self.config['max_papers_analyze']],
                substance=substance,
                outcome=outcome_category,
            )
            
            if len(papers) < 2:
                return self._create_error_entry(
                    error='Insufficient papers with complete metadata',
                    substance=substance,
                    outcome_category=outcome_category,
                    papers_found=len(pmids),
                    papers_with_metadata=len(papers)
                )
            
            print(f"‚úÖ Retrieved metadata for {len(papers)} papers")
            
            # Step 3: AI analysis (ONLY essential usage)
            print(f"\nü§ñ Step 3: AI analysis (minimal, cost-optimized)...")
            print(f"   üìä Using tiered approach to minimize AI dependency")
            
            ai_usage_start = datetime.now()
            
            analyses = await self.ai_analyzer.process_batch_optimized(
                papers=papers,
                substance=substance,
                outcome_category=outcome_category,
                relevance_threshold=0.6,
                confidence_threshold=0.7
            )
            
            ai_usage_time = (datetime.now() - ai_usage_start).total_seconds()
            
            if len(analyses) < 2:
                return self._create_error_entry(
                    error='Insufficient successful AI analyses',
                    substance=substance,
                    outcome_category=outcome_category,
                    papers_analyzed=len(papers),
                    successful_analyses=len(analyses),
                    ai_usage_seconds=ai_usage_time
                )
            
            print(f"‚úÖ AI analysis complete ({ai_usage_time:.1f}s)")
            
            # Step 4: Journal assessment (no AI needed)
            print(f"\nüèõÔ∏è Step 4: Journal quality assessment...")
            
            journal_assessments = {}
            for paper in papers:
                if paper.journal_issn:
                    assessment = await self.journal_db.assess_journal(
                        issn=paper.journal_issn,
                        title=paper.journal
                    )
                    journal_assessments[paper.pmid] = assessment
            
            print(f"‚úÖ Assessed {len(journal_assessments)} journals")
            
            # Step 5: Meta-analysis (no AI needed)
            print(f"\nüìä Step 5: Statistical meta-analysis...")
            
            entry = await generate_real_tervyx_entry(
                substance=substance,
                outcome_category=outcome_category,
                analyses=analyses,
                journal_assessments=journal_assessments
            )
            
            if 'error' in entry:
                return self._create_error_entry(
                    **entry,
                    pipeline_step='meta_analysis',
                    ai_usage_seconds=ai_usage_time
                )
            
            # Step 6: Add comprehensive metadata and attribution
            processing_time = (datetime.now() - start_time).total_seconds()
            
            enhanced_entry = self._add_enhanced_metadata(
                entry, pmids, papers, analyses, journal_assessments, 
                processing_time, ai_usage_time
            )
            
            print(f"\nüéâ SUCCESS: Generated TEL-{enhanced_entry['tier']} entry")
            print(f"üìä Evidence: {enhanced_entry['evidence_summary']['n_studies']} studies")
            print(f"‚è±Ô∏è Total time: {processing_time:.1f}s (AI: {ai_usage_time:.1f}s)")
            print(f"üìù Attribution: {AUTHOR_METADATA.author_info['name_english']} ({AUTHOR_METADATA.author_info['name_korean']})")
            
            return enhanced_entry
            
        except Exception as e:
            return self._create_error_entry(
                error=f'Pipeline failed: {str(e)}',
                error_type=type(e).__name__,
                traceback=traceback.format_exc(),
                substance=substance,
                outcome_category=outcome_category,
                processing_time=(datetime.now() - start_time).total_seconds()
            )
    
    def _add_enhanced_metadata(self, entry: Dict[str, Any], pmids: List[str], 
                             papers: List[PubMedPaper], analyses: List, 
                             journal_assessments: Dict, processing_time: float, 
                             ai_usage_time: float) -> Dict[str, Any]:
        """
        Add comprehensive metadata including author attribution and reproducibility info
        """
        
        # Base entry with author metadata
        enhanced_entry = {
            **entry,
            **self.author_metadata,  # Full author and citation metadata
        }
        
        # Enhanced pipeline metadata
        enhanced_entry['pipeline_metadata'] = {
            'processing_time_seconds': processing_time,
            'ai_usage_time_seconds': ai_usage_time,
            'ai_usage_percentage': (ai_usage_time / processing_time) * 100,
            'pubmed_search_results': len(pmids),
            'papers_with_metadata': len(papers),
            'successful_ai_analyses': len(analyses),
            'journal_assessments': len(journal_assessments),
            'pipeline_version': 'v2.0-enhanced-reproducible',
            'generation_timestamp': datetime.now().isoformat(),
            
            # Component versions and settings
            'components_used': {
                'pubmed_api': True,
                'cost_optimized_ai': True,
                'journal_quality_db': True,
                'reml_monte_carlo': True,
                'author_attribution': True
            },
            
            # Data sources with full provenance
            'data_sources': {
                'literature_search': 'PubMed E-utilities API',
                'ai_analysis': 'Gemini API (tiered cost-optimization)',
                'journal_quality': 'Multi-source aggregated database',
                'meta_analysis': 'Custom REML + Monte Carlo implementation',
                'methodology': f'TERVYX Protocol by {AUTHOR_METADATA.author_info["name_english"]}'
            },
            
            # Reproducibility guarantees
            'reproducibility': {
                'methodology_documented': True,
                'code_available': self.author_metadata['software']['codeRepository'],
                'data_provenance_tracked': True,
                'api_versions_recorded': True,
                'seed_reproducible': True,
                'contact_for_replication': AUTHOR_METADATA.author_info['email']
            }
        }
        
        # Loop reinforcement strategy
        enhanced_entry['loop_reinforcement'] = {
            'strategy': 'moneypuzzler.com ‚Üí TERVYX ‚Üí Publications ‚Üí Citations ‚Üí Recognition',
            'primary_contact': AUTHOR_METADATA.author_info['email'],
            'website_traffic': f"https://{AUTHOR_METADATA.author_info['website']}",
            'methodology_attribution': 'Always cite TERVYX Protocol methodology',
            'system_attribution': 'System developed by KIMGEONYEOB (ÍπÄÍ±¥ÏóΩ)',
            'recognition_loop': 'Each entry reinforces author visibility and expertise'
        }
        
        return enhanced_entry
    
    def _create_error_entry(self, **kwargs) -> Dict[str, Any]:
        """
        Create standardized error entry with full attribution
        """
        error_entry = {
            'error': kwargs.get('error', 'Unknown error'),
            'error_timestamp': datetime.now().isoformat(),
            **kwargs,
            **self.author_metadata  # Include attribution even in errors
        }
        
        print(f"\nüí• PIPELINE ERROR: {kwargs.get('error', 'Unknown error')}")
        
        return error_entry
    
    async def batch_generate_with_attribution(self, 
                                            substance_outcome_pairs: List[tuple],
                                            output_dir: str = "/home/user/webapp/entries_enhanced") -> Dict[str, Any]:
        """
        Generate multiple entries with full attribution and reproducibility tracking
        """
        print(f"\nüöÄ Enhanced Batch Generation: {len(substance_outcome_pairs)} entries")
        print(f"üë§ Author: {AUTHOR_METADATA.author_info['name_english']} ({AUTHOR_METADATA.author_info['name_korean']})")
        print(f"üìß Contact: {AUTHOR_METADATA.author_info['email']}")
        
        results = {
            'successful_entries': [],
            'failed_entries': [],
            'summary': {},
            'author_attribution': self.author_metadata,
            'batch_metadata': {
                'generated_by': f"{AUTHOR_METADATA.author_info['name_english']} ({AUTHOR_METADATA.author_info['name_korean']})",
                'contact_email': AUTHOR_METADATA.author_info['email'],
                'website': f"https://{AUTHOR_METADATA.author_info['website']}",
                'generation_timestamp': datetime.now().isoformat(),
                'methodology_citation': AUTHOR_METADATA._generate_citation()
            }
        }
        
        os.makedirs(output_dir, exist_ok=True)
        
        total_ai_time = 0
        total_processing_time = 0
        
        for i, (substance, outcome_category) in enumerate(substance_outcome_pairs):
            print(f"\n{'='*60}")
            print(f"Processing {i+1}/{len(substance_outcome_pairs)}: {substance} + {outcome_category}")
            print(f"{'='*60}")
            
            try:
                entry = await self.generate_entry_with_attribution(substance, outcome_category)
                
                if 'error' in entry:
                    results['failed_entries'].append({
                        'substance': substance,
                        'outcome_category': outcome_category,
                        'error': entry['error']
                    })
                else:
                    # Track AI usage
                    if 'pipeline_metadata' in entry:
                        total_ai_time += entry['pipeline_metadata'].get('ai_usage_time_seconds', 0)
                        total_processing_time += entry['pipeline_metadata'].get('processing_time_seconds', 0)
                    
                    # Save with enhanced attribution
                    entry_dir = os.path.join(output_dir, f"nutrient/{substance}/{outcome_category}/v2-enhanced")
                    os.makedirs(entry_dir, exist_ok=True)
                    
                    # Save main entry (clean version)
                    clean_entry = {k: v for k, v in entry.items() 
                                 if k not in ['real_studies', 'pipeline_metadata']}
                    
                    with open(os.path.join(entry_dir, "entry.jsonld"), 'w') as f:
                        json.dump(clean_entry, f, indent=2, ensure_ascii=False)
                    
                    # Save detailed analysis with full metadata
                    with open(os.path.join(entry_dir, "analysis_detailed.json"), 'w') as f:
                        json.dump(entry, f, indent=2, default=str, ensure_ascii=False)
                    
                    # Save attribution info separately for easy access
                    with open(os.path.join(entry_dir, "attribution.json"), 'w') as f:
                        json.dump({
                            'author': entry['author'],
                            'citation': entry['citation'],
                            'methodology': entry['methodology'],
                            'contact': AUTHOR_METADATA.author_info['email'],
                            'website': f"https://{AUTHOR_METADATA.author_info['website']}"
                        }, f, indent=2, ensure_ascii=False)
                    
                    results['successful_entries'].append({
                        'substance': substance,
                        'outcome_category': outcome_category,
                        'tier': entry['tier'],
                        'label': entry['label'],
                        'n_studies': entry['evidence_summary']['n_studies'],
                        'author_attribution': f"{AUTHOR_METADATA.author_info['name_english']} ({AUTHOR_METADATA.author_info['name_korean']})"
                    })
                    
                    print(f"‚úÖ Saved with full attribution to {entry_dir}")
                
            except Exception as e:
                results['failed_entries'].append({
                    'substance': substance,
                    'outcome_category': outcome_category,
                    'error': str(e)
                })
            
            # Rate limiting
            if i < len(substance_outcome_pairs) - 1:
                print(f"‚è≥ Waiting 30 seconds before next entry...")
                await asyncio.sleep(30)
        
        # Calculate summary with AI usage statistics
        successful = len(results['successful_entries'])
        failed = len(results['failed_entries'])
        
        results['summary'] = {
            'total_attempted': len(substance_outcome_pairs),
            'successful': successful,
            'failed': failed,
            'success_rate': successful / len(substance_outcome_pairs) * 100,
            'total_ai_usage_seconds': total_ai_time,
            'total_processing_seconds': total_processing_time,
            'ai_usage_percentage': (total_ai_time / max(1, total_processing_time)) * 100,
            'average_ai_time_per_entry': total_ai_time / max(1, successful),
            'author_attribution': f"{AUTHOR_METADATA.author_info['name_english']} ({AUTHOR_METADATA.author_info['name_korean']})"
        }
        
        # Save batch results with attribution
        with open(os.path.join(output_dir, "batch_results.json"), 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüìä ENHANCED BATCH COMPLETE:")
        print(f"   ‚úÖ Successful: {successful}/{len(substance_outcome_pairs)} ({successful/len(substance_outcome_pairs)*100:.1f}%)")
        print(f"   ü§ñ Total AI usage: {total_ai_time:.1f}s ({(total_ai_time/max(1,total_processing_time))*100:.1f}% of processing time)")
        print(f"   üë§ All entries attributed to: {AUTHOR_METADATA.author_info['name_english']} ({AUTHOR_METADATA.author_info['name_korean']})")
        print(f"   üìß Contact: {AUTHOR_METADATA.author_info['email']}")
        
        return results

# Example usage
async def test_enhanced_pipeline():
    """Test the enhanced pipeline with attribution"""
    
    email = os.getenv('TERVYX_EMAIL', 'moneypuzzler@gmail.com')
    gemini_key = os.getenv('GEMINI_API_KEY')
    
    if not gemini_key:
        print("‚ö†Ô∏è Set GEMINI_API_KEY environment variable")
        return
    
    # Initialize enhanced pipeline
    pipeline = EnhancedTERVYXPipeline(
        email=email,
        gemini_api_key=gemini_key,
        minimize_ai_usage=True  # Focus on reproducibility
    )
    
    print("üß™ Testing enhanced pipeline with attribution...")
    
    entry = await pipeline.generate_entry_with_attribution("melatonin", "sleep")
    
    if 'error' not in entry:
        print(f"\n‚úÖ SUCCESS with full attribution!")
        print(f"   Author: {entry['author']['name']} ({entry['author']['alternateName']})")
        print(f"   Contact: {entry['author']['email']}")
        print(f"   Citation: {entry['citation']}")
        print(f"   Website: {entry['author']['url']}")
    else:
        print(f"‚ùå Error: {entry['error']}")
    
    return entry

if __name__ == "__main__":
    asyncio.run(test_enhanced_pipeline())